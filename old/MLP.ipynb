{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ef44029",
   "metadata": {},
   "source": [
    "# MLP fully connected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7bfa5d5-6129-4142-bb7d-5572b78c0876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 01:48:23.336584: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# pretrain MLP using the new dataloader\n",
    "\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
    "\n",
    "from typing import List\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import preprocessing\n",
    "from tqdm import tqdm\n",
    "import keras_core as keras\n",
    "from keras_core import layers\n",
    "from keras_core import ops\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback, WandbMetricsLogger\n",
    "\n",
    "from data_utils import preprocess_data, H5Dataset, H5Dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7372878f-3077-4cc3-9195-8f7a349bcf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\"\n",
    "train_dir_preprocess = \"/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/\"\n",
    "train_filepaths = [train_dir + x for x in os.listdir(train_dir)]\n",
    "# preprocess_data(train_filepaths, train_dir_preprocess, force=False)\n",
    "train_preprocess_file_names = os.listdir(train_dir_preprocess)\n",
    "train_preprocess_filepaths = [train_dir_preprocess + name for name in train_preprocess_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54bd753e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_3.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_4.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_1.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_7.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/asdf.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_10.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_6.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_9.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_5.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_11.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_12.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_8.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_2.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_14.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_13.h5',\n",
       " '/global/ml4hep/spss/mfong/transfer_learning/delphes_train_processed/preprocessed_train_0.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_preprocess_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7290b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_len = 0\n",
    "# for filepath in train_preprocess_filepaths:\n",
    "#   with h5py.File(filepath, \"r\") as f:\n",
    "#     total_len += len(f[\"labels\"])\n",
    "#     print(\"data shape =\", f[\"data\"].shape)\n",
    "#     print(\"labels shape =\", f[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ff82d9-c59b-48d1-a0c7-8022029ad8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num train samples: 20000000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = H5Dataset2(train_preprocess_filepaths[0:4], transform=None)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=False)\n",
    "print(\"Num train samples:\", len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52ae5aa3-34a1-4fc0-8799-eabcbc1b23c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num test samples 5000000\n"
     ]
    }
   ],
   "source": [
    "test_dataset = H5Dataset2(train_preprocess_filepaths[5:6], transform=None)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
    "print(\"Num test samples\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3da501a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea962c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmingfong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/global/home/users/mfong/git/transfer-learning/wandb/run-20231025_014850-431n8wap</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mingfong/delphes_pretrain/runs/431n8wap' target=\"_blank\">MLP_delphes</a></strong> to <a href='https://wandb.ai/mingfong/delphes_pretrain' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mingfong/delphes_pretrain' target=\"_blank\">https://wandb.ai/mingfong/delphes_pretrain</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mingfong/delphes_pretrain/runs/431n8wap' target=\"_blank\">https://wandb.ai/mingfong/delphes_pretrain/runs/431n8wap</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = {\n",
    "  \"epochs\": 400,\n",
    "  \"batch_size\": 1024,\n",
    "  \"learning_rate\": 0.001,\n",
    "  \"optimizer\": \"adam\",\n",
    "  \"loss\": \"binary_crossentropy\",\n",
    "  \"train_samples\": len(train_dataset),\n",
    "  \"test_samples\": len(test_dataset),\n",
    "}\n",
    "wandb_run = wandb.init(\n",
    "  project=\"delphes_pretrain\",\n",
    "  name=f\"MLP_delphes\",\n",
    "  config=config, reinit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7269e5-da92-456d-a93a-7818906d28c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 01:49:05.079995: W external/xla/xla/service/gpu/nvptx_compiler.cc:596] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.3.52). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "  keras.Input(shape=(200 * 7,)),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(8, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid'),\n",
    "])\n",
    "model.compile(loss=config[\"loss\"], optimizer=config[\"optimizer\"], metrics=[\"accuracy\", \"AUC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf47922e-416c-438a-8e99-0e6db5d899bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Unable to log learning rate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   34/19532\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30:54:01\u001b[0m 6s/step - accuracy: 0.6282 - auc: 0.6431 - loss: 0.9408"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/global/home/users/mfong/git/transfer-learning/MLP.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train_history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m   train_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m   validation_data\u001b[39m=\u001b[39;49mtest_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m   batch_size\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m   epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m\"\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m   callbacks\u001b[39m=\u001b[39;49m[WandbMetricsLogger(log_freq\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbatch\u001b[39;49m\u001b[39m\"\u001b[39;49m)],\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://tunnel%2Bml4hep4/global/home/users/mfong/git/transfer-learning/MLP.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# model.evaluate(test_dataloader)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/keras_core/src/utils/traceback_utils.py:118\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    119\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/keras_core/src/backend/jax/trainer.py:372\u001b[0m, in \u001b[0;36mJAXTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m optimizer_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mvariables\n\u001b[1;32m    370\u001b[0m metrics_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics_variables\n\u001b[0;32m--> 372\u001b[0m \u001b[39mfor\u001b[39;00m step, data \u001b[39min\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39menumerate_epoch(return_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnp\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    373\u001b[0m     \u001b[39m# Callbacks\u001b[39;00m\n\u001b[1;32m    374\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m    376\u001b[0m     \u001b[39m# Train step\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/keras_core/src/trainers/epoch_iterator.py:198\u001b[0m, in \u001b[0;36mEpochIterator.enumerate_epoch\u001b[0;34m(self, return_type)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[39myield\u001b[39;00m step \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(buffer) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, buffer\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     \u001b[39mfor\u001b[39;00m step, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_iterator(return_type)):\n\u001b[1;32m    199\u001b[0m         buffer\u001b[39m.\u001b[39mappend(data)\n\u001b[1;32m    200\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(buffer) \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_per_execution:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/keras_core/src/trainers/data_adapters/torch_data_adapter.py:24\u001b[0m, in \u001b[0;36mTorchDataLoaderAdapter.get_numpy_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_numpy_iterator\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 24\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataloader:\n\u001b[1;32m     25\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mtuple\u001b[39m(tree\u001b[39m.\u001b[39mmap_structure(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mnumpy(), batch))\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/git/transfer-learning/data_utils.py:176\u001b[0m, in \u001b[0;36mH5Dataset2.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mwith\u001b[39;00m h5py\u001b[39m.\u001b[39mFile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepaths[filepath_idx], \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m    175\u001b[0m   labels \u001b[39m=\u001b[39m file[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLABEL_KEY][sample_idx]\n\u001b[0;32m--> 176\u001b[0m   data \u001b[39m=\u001b[39m file[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mDATA_KEY][sample_idx]\n\u001b[1;32m    178\u001b[0m \u001b[39m# if self.preprocessed:\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[39m#   data = self.opened_file[self.DATA_KEY][sample_idx]\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m# if self.transform:\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39m#   data = self.transform(data)\u001b[39;00m\n\u001b[1;32m    191\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(data)\u001b[39m.\u001b[39mflatten()\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml4hep4/lib/python3.8/site-packages/h5py/_hl/dataset.py:758\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m    756\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fast_read_ok \u001b[39mand\u001b[39;00m (new_dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    757\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 758\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fast_reader\u001b[39m.\u001b[39;49mread(args)\n\u001b[1;32m    759\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    760\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "  train_dataloader,\n",
    "  validation_data=test_dataloader,\n",
    "  batch_size=config[\"batch_size\"],\n",
    "  epochs=config[\"epochs\"],\n",
    "  callbacks=[WandbMetricsLogger(log_freq=\"batch\")],\n",
    ")\n",
    "# model.evaluate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97c08f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/anaconda3/envs/ml4hep4/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "554e8357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>▁▄▆▇███▇</td></tr><tr><td>batch/auc_2</td><td>▁▅▆▇████</td></tr><tr><td>batch/batch_step</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>batch/loss</td><td>█▅▃▃▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>batch/accuracy</td><td>0.62622</td></tr><tr><td>batch/auc_2</td><td>0.66354</td></tr><tr><td>batch/batch_step</td><td>7</td></tr><tr><td>batch/loss</td><td>0.95322</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">MLP_delphes</strong> at: <a href='https://wandb.ai/mingfong/fullsim_MLP/runs/sggjquw1' target=\"_blank\">https://wandb.ai/mingfong/fullsim_MLP/runs/sggjquw1</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231025_012820-sggjquw1/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
