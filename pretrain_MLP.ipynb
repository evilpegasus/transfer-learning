{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining `num_rows` vs transfered performance\n",
    "Pretrain fastsim weights for 1M, 2M, 4M, 8M, ... rows\n",
    "\n",
    "Then transfer and do fixed fullsim transfer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 02:16:18.419644: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-05-18 02:16:18.444614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:41:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2023-05-18 02:16:18.445006: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-05-18 02:16:18.447146: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-05-18 02:16:18.449358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-05-18 02:16:18.449766: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-05-18 02:16:18.451847: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-05-18 02:16:18.453017: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-05-18 02:16:18.457151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-05-18 02:16:18.458286: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# https://gitlab.cern.ch/atlas/ATLAS-top-tagging-open-data/-/blob/master/preprocessing.py\n",
    "import preprocessing\n",
    "\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_0.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names = os.listdir(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train\")\n",
    "# for i in range(7, 15):\n",
    "for i in range(1, 15):\n",
    "    train_file_names.remove(f\"train_{i}.h5\")\n",
    "train_file_names\n",
    "# f = h5py.File('/clusterfs/ml4hep/mfong/transfer_learning/delphes_train.h5', 'r')\n",
    "# f2 = h5py.File('/clusterfs/ml4hep/mfong/transfer_learning/delphes_test.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_file = []\n",
    "for train_file_name in train_file_names:\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    num_samples_per_file.append(f[\"fjet_clus_eta\"].shape[0])\n",
    "num_samples = sum(num_samples_per_file)\n",
    "feature_keys = ['fjet_clus_eta', 'fjet_clus_phi', 'fjet_clus_pt', 'fjet_clus_E']\n",
    "num_features = 0\n",
    "for k in feature_keys:\n",
    "    num_features += f[k].shape[1]\n",
    "# x = np.empty((num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/global/home/users/mfong/git/transfer-learning/preprocessing.py:90: RuntimeWarning: divide by zero encountered in log\n",
      "  log_pt = np.log(pt)\n",
      "/global/home/users/mfong/git/transfer-learning/preprocessing.py:91: RuntimeWarning: divide by zero encountered in log\n",
      "  log_energy = np.log(energy)\n",
      "/global/home/users/mfong/git/transfer-learning/preprocessing.py:98: RuntimeWarning: divide by zero encountered in log\n",
      "  lognorm_pt = np.log(pt / sum_pt[:,np.newaxis])\n",
      "/global/home/users/mfong/git/transfer-learning/preprocessing.py:99: RuntimeWarning: divide by zero encountered in log\n",
      "  lognorm_energy = np.log(energy / sum_energy[:,np.newaxis])\n",
      "1it [04:31, 271.89s/it]\n"
     ]
    }
   ],
   "source": [
    "current_row = 0\n",
    "for train_file_name, current_num_samples in tqdm(zip(train_file_names, num_samples_per_file)):\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    \n",
    "    # preprocess\n",
    "    data_dict = {k:v for k, v in f.items() if k in feature_keys}\n",
    "    x = preprocessing.constituent(data_dict, 200)       # TODO need to put these into preallocated x array\n",
    "    x = x.reshape(x.shape[0], x.shape[1]*x.shape[2])\n",
    "    # x = preprocessing.high_level(x)\n",
    "    \n",
    "    # x[current_row:current_row+current_num_samples] = np.concatenate([f[k] for k in feature_keys], axis=1)\n",
    "    current_row += current_num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fjet_clus_E (5000000, 200)\n",
      "fjet_clus_eta (5000000, 200)\n",
      "fjet_clus_phi (5000000, 200)\n",
      "fjet_clus_pt (5000000, 200)\n",
      "fjet_eta (5000000,)\n",
      "fjet_m (5000000,)\n",
      "fjet_phi (5000000,)\n",
      "fjet_pt (5000000,)\n",
      "labels (5000000,)\n",
      "training_weights (5000000,)\n"
     ]
    }
   ],
   "source": [
    "feature_keys = ['fjet_clus_eta', 'fjet_clus_phi', 'fjet_clus_pt', 'fjet_clus_E']\n",
    "for k in f.keys():\n",
    "    print(k, f[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "y = np.zeros((num_samples))\n",
    "current_row = 0\n",
    "for train_file_name, current_num_samples in tqdm(zip(train_file_names, num_samples_per_file)):\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    y[current_row:current_row+current_num_samples] = f[\"labels\"][:]\n",
    "    current_row += current_num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000, 1400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(y)\n",
    "# num_train_samples = int(0.8 * num_samples)\n",
    "num_train_samples = num_samples - 2000000       # save 2M rows for test data\n",
    "x_train = x[:num_train_samples]\n",
    "y_train = y[:num_train_samples]\n",
    "\n",
    "x_test = x[num_train_samples:]\n",
    "y_test = y[num_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_train[:2000000])   # only use first 2M otherwise takes too long\n",
    "\n",
    "# x_train = scaler.transform(x_train, copy=False)\n",
    "# x_test = scaler.transform(x_test, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"pretrain_MLP.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.15.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/global/home/users/mfong/git/transfer-learning/wandb/run-20230518_023235-n2sc88on</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mingfong/preprocess_pretrain_MLP/runs/n2sc88on' target=\"_blank\">preprocess_fastsim_MLP_1M_rows</a></strong> to <a href='https://wandb.ai/mingfong/preprocess_pretrain_MLP' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mingfong/preprocess_pretrain_MLP' target=\"_blank\">https://wandb.ai/mingfong/preprocess_pretrain_MLP</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mingfong/preprocess_pretrain_MLP/runs/n2sc88on' target=\"_blank\">https://wandb.ai/mingfong/preprocess_pretrain_MLP/runs/n2sc88on</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 02:32:46.253687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3901/3907 [============================>.] - ETA: 0s - loss: 0.4409 - accuracy: 0.7940"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 02:34:48.746270: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 11200000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /global/home/users/mfong/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1813: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 02:35:06.467060: W tensorflow/python/util/util.cc:329] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /global/home/users/mfong/git/transfer-learning/wandb/run-20230518_023235-n2sc88on/files/model-best/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (/global/home/users/mfong/git/transfer-learning/wandb/run-20230518_023235-n2sc88on/files/model-best)... Done. 0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3907/3907 [==============================] - 28s 7ms/step - loss: 0.4409 - accuracy: 0.7940 - val_loss: 0.4229 - val_accuracy: 0.8050\n",
      "Epoch 2/800\n",
      "3891/3907 [============================>.] - ETA: 0s - loss: 0.4095 - accuracy: 0.8132"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 02:35:16.967475: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 11200000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3907/3907 [==============================] - 28s 7ms/step - loss: 0.4094 - accuracy: 0.8132 - val_loss: 0.4500 - val_accuracy: 0.7903\n",
      "Epoch 3/800\n",
      "3510/3907 [=========================>....] - ETA: 1s - loss: 0.3977 - accuracy: 0.8205"
     ]
    }
   ],
   "source": [
    "NUM_PRETRAIN_ROWS_LIST = [1000000, 2000000, 4000000]#, 8000000, 16000000, 32000000]\n",
    "# config = wandb.config\n",
    "# config.batch_size = 256\n",
    "config = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 800,\n",
    "}\n",
    "for num_pretrain_rows in NUM_PRETRAIN_ROWS_LIST:\n",
    "    # config.num_pretrain_rows = num_pretrain_rows\n",
    "    config[\"num_pretrain_rows\"] = num_pretrain_rows\n",
    "    run = wandb.init(project=\"preprocess_pretrain_MLP\", name=f\"preprocess_fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows\", config=config, reinit=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(x_train.shape[1],), activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        x_train[:config[\"num_pretrain_rows\"]],\n",
    "        y_train[:config[\"num_pretrain_rows\"]],\n",
    "        epochs=config[\"epochs\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[wandb.keras.WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"acc\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(f\"Fastsim MLP ({int(num_pretrain_rows / 1000000)}M Rows)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/preprocess_fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows_acc.png\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.title(f\"Fastsim MLP ({int(num_pretrain_rows / 1000000)}M Rows)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/preprocess_fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows_loss.png\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.save_weights(f\"models/preprocess_fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows.h5\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
