{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining `num_rows` vs transfered performance\n",
    "Pretrain fastsim weights for 1M, 2M, 4M, 8M, ... rows\n",
    "\n",
    "Then transfer and do fixed fullsim transfer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 01:17:48.895936: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-05-18 01:17:48.916961: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:41:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2023-05-18 01:17:48.917369: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-05-18 01:17:48.919058: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-05-18 01:17:48.920764: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-05-18 01:17:48.921126: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-05-18 01:17:48.922687: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-05-18 01:17:48.923570: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-05-18 01:17:48.926567: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-05-18 01:17:48.927566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "# https://gitlab.cern.ch/atlas/ATLAS-top-tagging-open-data/-/blob/master/preprocessing.py\n",
    "import preprocessing\n",
    "\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_0.h5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names = os.listdir(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train\")\n",
    "# for i in range(7, 15):\n",
    "for i in range(1, 15):\n",
    "    train_file_names.remove(f\"train_{i}.h5\")\n",
    "train_file_names\n",
    "# f = h5py.File('/clusterfs/ml4hep/mfong/transfer_learning/delphes_train.h5', 'r')\n",
    "# f2 = h5py.File('/clusterfs/ml4hep/mfong/transfer_learning/delphes_test.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_file = []\n",
    "for train_file_name in train_file_names:\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    num_samples_per_file.append(f[\"fjet_clus_eta\"].shape[0])\n",
    "num_samples = sum(num_samples_per_file)\n",
    "feature_keys = ['fjet_clus_eta', 'fjet_clus_phi', 'fjet_clus_pt', 'fjet_clus_E']\n",
    "num_features = 0\n",
    "for k in feature_keys:\n",
    "    num_features += f[k].shape[1]\n",
    "x = np.empty((num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fjet_clus_E': <HDF5 dataset \"fjet_clus_E\": shape (5000000, 200), type \"<f4\">, 'fjet_clus_eta': <HDF5 dataset \"fjet_clus_eta\": shape (5000000, 200), type \"<f4\">, 'fjet_clus_phi': <HDF5 dataset \"fjet_clus_phi\": shape (5000000, 200), type \"<f4\">, 'fjet_clus_pt': <HDF5 dataset \"fjet_clus_pt\": shape (5000000, 200), type \"<f4\">}\n",
      "dict_keys(['fjet_clus_E', 'fjet_clus_eta', 'fjet_clus_phi', 'fjet_clus_pt'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/home/users/mfong/git/transfer-learning/preprocessing.py:90: RuntimeWarning: divide by zero encountered in log\n",
      "  log_pt = np.log(pt)\n",
      "/global/home/users/mfong/git/transfer-learning/preprocessing.py:91: RuntimeWarning: divide by zero encountered in log\n",
      "  log_energy = np.log(energy)\n",
      "/global/home/users/mfong/git/transfer-learning/preprocessing.py:98: RuntimeWarning: divide by zero encountered in log\n",
      "  lognorm_pt = np.log(pt / sum_pt[:,np.newaxis])\n",
      "/global/home/users/mfong/git/transfer-learning/preprocessing.py:99: RuntimeWarning: divide by zero encountered in log\n",
      "  lognorm_energy = np.log(energy / sum_energy[:,np.newaxis])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00000000e+00,  0.00000000e+00,  1.37518911e+01, ...,\n",
       "         -1.18225360e+00, -1.21085036e+00,  0.00000000e+00],\n",
       "        [-1.16415322e-10, -1.87876681e-03,  1.28428078e+01, ...,\n",
       "         -2.09133649e+00, -2.12047672e+00,  1.87876681e-03],\n",
       "        [ 5.57777006e-03,  5.05658705e-03,  1.21156282e+01, ...,\n",
       "         -2.81851649e+00, -2.84220719e+00,  7.52865151e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  1.40549526e+01, ...,\n",
       "         -4.46708560e-01, -4.52646613e-01,  0.00000000e+00],\n",
       "        [-7.45058060e-09, -9.94883850e-02,  1.19757719e+01, ...,\n",
       "         -2.52588868e+00, -2.52433801e+00,  9.94883850e-02],\n",
       "        [ 6.65695686e-03, -1.30128721e-02,  1.13603592e+01, ...,\n",
       "         -3.14130187e+00, -3.14649320e+00,  1.46167688e-02],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  1.35544252e+01, ...,\n",
       "         -1.10652781e+00, -1.10798907e+00,  0.00000000e+00],\n",
       "        [-4.65661287e-10, -8.34086444e-03,  1.29748144e+01, ...,\n",
       "         -1.68613851e+00, -1.68607056e+00,  8.34086444e-03],\n",
       "        [ 1.09352157e-01,  4.17508259e-02,  1.23188295e+01, ...,\n",
       "         -2.34212303e+00, -2.36162591e+00,  1.17051378e-01],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.00000000e+00,  0.00000000e+00,  1.31574078e+01, ...,\n",
       "         -1.76688981e+00, -1.82187212e+00,  0.00000000e+00],\n",
       "        [ 5.82076609e-11, -3.66730778e-03,  1.29897738e+01, ...,\n",
       "         -1.93452477e+00, -1.98929787e+00,  3.66730778e-03],\n",
       "        [ 1.17521000e-03, -3.38649936e-03,  1.25675964e+01, ...,\n",
       "         -2.35670161e+00, -2.41114521e+00,  3.58461938e-03],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-0.00000000e+00,  0.00000000e+00,  1.31033535e+01, ...,\n",
       "         -1.18422890e+00, -1.17627752e+00,  0.00000000e+00],\n",
       "        [ 2.56113708e-09, -2.77180504e-02,  1.15504522e+01, ...,\n",
       "         -2.73713017e+00, -2.71819496e+00,  2.77180504e-02],\n",
       "        [ 4.20757353e-01,  1.13194928e-01,  1.15328951e+01, ...,\n",
       "         -2.75468802e+00, -2.78887773e+00,  4.35717613e-01],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]],\n",
       "\n",
       "       [[-0.00000000e+00, -0.00000000e+00,  1.40484695e+01, ...,\n",
       "         -8.59514415e-01, -8.42614889e-01,  0.00000000e+00],\n",
       "        [-1.49011612e-08, -1.01207763e-01,  1.32505560e+01, ...,\n",
       "         -1.65742815e+00, -1.69078052e+00,  1.01207763e-01],\n",
       "        [ 7.23728538e-03, -9.88686159e-02,  1.25651588e+01, ...,\n",
       "         -2.34282565e+00, -2.37869287e+00,  9.91331562e-02],\n",
       "        ...,\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "        [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict = {k:v for k, v in f.items() if k in feature_keys}\n",
    "print(data_dict)\n",
    "print(data_dict.keys())\n",
    "preprocessing.constituent(data_dict, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000000, 200)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict[feature_keys[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m f \u001b[38;5;241m=\u001b[39m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m train_file_name, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# preprocess\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m data_dict \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_keys\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(data_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m()\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/h5py/_hl/group.py:264\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     oid \u001b[38;5;241m=\u001b[39m h5o\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_e\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m, lapl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lapl)\n\u001b[1;32m    266\u001b[0m otype \u001b[38;5;241m=\u001b[39m h5i\u001b[38;5;241m.\u001b[39mget_type(oid)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m otype \u001b[38;5;241m==\u001b[39m h5i\u001b[38;5;241m.\u001b[39mGROUP:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.8/site-packages/h5py/_hl/base.py:137\u001b[0m, in \u001b[0;36mCommonStateObject._e\u001b[0;34m(self, name, lcpl)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         name \u001b[38;5;241m=\u001b[39m \u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    138\u001b[0m         coding \u001b[38;5;241m=\u001b[39m h5t\u001b[38;5;241m.\u001b[39mCSET_ASCII\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "current_row = 0\n",
    "for train_file_name, current_num_samples in tqdm(zip(train_file_names, num_samples_per_file)):\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    \n",
    "    # preprocess\n",
    "    data_dict = {k:v for k, v in f.items() if k in feature_keys}\n",
    "    print(data_dict.keys())\n",
    "    raise Exception()\n",
    "    \n",
    "    x[current_row:current_row+current_num_samples] = np.concatenate([f[k] for k in feature_keys], axis=1)\n",
    "    current_row += current_num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = ['fjet_clus_eta', 'fjet_clus_phi', 'fjet_clus_pt', 'fjet_clus_E']\n",
    "for k in f.keys():\n",
    "    print(k, f[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((num_samples))\n",
    "current_row = 0\n",
    "for train_file_name, current_num_samples in tqdm(zip(train_file_names, num_samples_per_file)):\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    y[current_row:current_row+current_num_samples] = f[\"labels\"][:]\n",
    "    current_row += current_num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(y)\n",
    "# num_train_samples = int(0.8 * num_samples)\n",
    "num_train_samples = num_samples - 2000000       # save 2M rows for test data\n",
    "x_train = x[:num_train_samples]\n",
    "y_train = y[:num_train_samples]\n",
    "\n",
    "x_test = x[num_train_samples:]\n",
    "y_test = y[num_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train[:2000000])   # only use first 2M otherwise takes too long\n",
    "\n",
    "x_train = scaler.transform(x_train, copy=False)\n",
    "x_test = scaler.transform(x_test, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"pretrain_MLP.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_PRETRAIN_ROWS_LIST = [1000000, 2000000, 4000000, 8000000, 16000000, 32000000]\n",
    "# config = wandb.config\n",
    "# config.batch_size = 256\n",
    "config = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 400,\n",
    "}\n",
    "for num_pretrain_rows in NUM_PRETRAIN_ROWS_LIST:\n",
    "    # config.num_pretrain_rows = num_pretrain_rows\n",
    "    config[\"num_pretrain_rows\"] = num_pretrain_rows\n",
    "    run = wandb.init(project=\"pretrain_MLP\", name=f\"fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows\", config=config, reinit=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(600,), activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        x_train[:config[\"num_pretrain_rows\"]],\n",
    "        y_train[:config[\"num_pretrain_rows\"]],\n",
    "        epochs=config[\"epochs\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[wandb.keras.WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"acc\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(f\"Fastsim MLP ({int(num_pretrain_rows / 1000000)}M Rows)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows_acc.png\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.title(f\"Fastsim MLP ({int(num_pretrain_rows / 1000000)}M Rows)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows_loss.png\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.save_weights(f\"models/fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows.h5\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
