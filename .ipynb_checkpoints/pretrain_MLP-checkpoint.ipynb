{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining `num_rows` vs transfered performance\n",
    "Pretrain fastsim weights for 1M, 2M, 4M, 8M, ... rows\n",
    "\n",
    "Then transfer and do fixed fullsim transfer training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 21:29:48.289922: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2023-04-27 21:29:48.326063: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:41:00.0 name: NVIDIA A40 computeCapability: 8.6\n",
      "coreClock: 1.74GHz coreCount: 84 deviceMemorySize: 44.56GiB deviceMemoryBandwidth: 648.29GiB/s\n",
      "2023-04-27 21:29:48.327948: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
      "2023-04-27 21:29:48.375134: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10\n",
      "2023-04-27 21:29:48.413230: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2023-04-27 21:29:48.471222: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2023-04-27 21:29:48.507673: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10\n",
      "2023-04-27 21:29:48.533571: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10\n",
      "2023-04-27 21:29:48.611000: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
      "2023-04-27 21:29:48.612484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "np.random.seed(8)\n",
    "tf.random.set_seed(8)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_5.h5',\n",
       " 'train_0.h5',\n",
       " 'train_4.h5',\n",
       " 'train_1.h5',\n",
       " 'train_6.h5',\n",
       " 'train_2.h5',\n",
       " 'train_3.h5']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_file_names = os.listdir(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train\")\n",
    "for i in range(7, 15):\n",
    "    train_file_names.remove(f\"train_{i}.h5\")\n",
    "train_file_names\n",
    "# f = h5py.File('/clusterfs/ml4hep/mfong/transfer_learning/delphes_train.h5', 'r')\n",
    "# f2 = h5py.File('/clusterfs/ml4hep/mfong/transfer_learning/delphes_test.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_per_file = []\n",
    "for train_file_name in train_file_names:\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    num_samples_per_file.append(f[\"fjet_clus_eta\"].shape[0])\n",
    "num_samples = sum(num_samples_per_file)\n",
    "feature_keys = ['fjet_clus_eta', 'fjet_clus_phi', 'fjet_clus_pt']\n",
    "num_features = 0\n",
    "for k in feature_keys:\n",
    "    num_features += f[k].shape[1]\n",
    "x = np.empty((num_samples, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [02:19, 139.90s/it]"
     ]
    }
   ],
   "source": [
    "current_row = 0\n",
    "for train_file_name, current_num_samples in tqdm(zip(train_file_names, num_samples_per_file)):\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    x[current_row:current_row+current_num_samples] = np.concatenate([f[k] for k in feature_keys], axis=1)\n",
    "    current_row += current_num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_keys = ['fjet_clus_eta', 'fjet_clus_phi', 'fjet_clus_pt']\n",
    "for k in f.keys():\n",
    "    print(k, f[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros((num_samples))\n",
    "current_row = 0\n",
    "for train_file_name, current_num_samples in tqdm(zip(train_file_names, num_samples_per_file)):\n",
    "    f = h5py.File(\"/global/ml4hep/spss/mfong/transfer_learning/delphes_train/\" + train_file_name, 'r')\n",
    "    y[current_row:current_row+current_num_samples] = f[\"labels\"][:]\n",
    "    current_row += current_num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(y)\n",
    "# num_train_samples = int(0.8 * num_samples)\n",
    "num_train_samples = num_samples - 2000000       # save 2M rows for test data\n",
    "x_train = x[:num_train_samples]\n",
    "y_train = y[:num_train_samples]\n",
    "\n",
    "x_test = x[num_train_samples:]\n",
    "y_test = y[num_train_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train[:2000000])   # only use first 2M otherwise takes too long\n",
    "\n",
    "x_train = scaler.transform(x_train, copy=False)\n",
    "x_test = scaler.transform(x_test, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"pretrain_MLP.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_PRETRAIN_ROWS_LIST = [1000000, 2000000, 4000000, 8000000, 16000000, 32000000]\n",
    "# config = wandb.config\n",
    "# config.batch_size = 256\n",
    "config = {\n",
    "    \"batch_size\": 256,\n",
    "    \"epochs\": 400,\n",
    "}\n",
    "for num_pretrain_rows in NUM_PRETRAIN_ROWS_LIST:\n",
    "    # config.num_pretrain_rows = num_pretrain_rows\n",
    "    config[\"num_pretrain_rows\"] = num_pretrain_rows\n",
    "    run = wandb.init(project=\"pretrain_MLP\", name=f\"fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows\", config=config, reinit=True)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_shape=(600,), activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        x_train[:config[\"num_pretrain_rows\"]],\n",
    "        y_train[:config[\"num_pretrain_rows\"]],\n",
    "        epochs=config[\"epochs\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        validation_data=(x_test, y_test),\n",
    "        callbacks=[wandb.keras.WandbCallback()]\n",
    "    )\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"accuracy\"], label=\"acc\")\n",
    "    plt.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "    plt.title(f\"Fastsim MLP ({int(num_pretrain_rows / 1000000)}M Rows)\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows_acc.png\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(history.history[\"loss\"], label=\"loss\")\n",
    "    plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.title(f\"Fastsim MLP ({int(num_pretrain_rows / 1000000)}M Rows)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"output/fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows_loss.png\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    model.save_weights(f\"models/fastsim_MLP_{int(num_pretrain_rows / 1000000)}M_rows.h5\")\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
